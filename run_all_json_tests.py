#!/usr/bin/env python3
"""
JSONæµ‹è¯•è¿è¡Œå™¨
è¿è¡Œæ‰€æœ‰JSONç›¸å…³çš„æµ‹è¯•æ–‡ä»¶
"""

import sys
import time
import argparse
from loguru import logger
import subprocess
import os

def run_test_file(test_file, args=None):
    """è¿è¡Œå•ä¸ªæµ‹è¯•æ–‡ä»¶"""
    logger.info(f"Running test file: {test_file}")
    
    if not os.path.exists(test_file):
        logger.error(f"Test file not found: {test_file}")
        return False
    
    try:
        start_time = time.time()
        
        # æ„å»ºå‘½ä»¤
        cmd = [sys.executable, test_file]
        if args:
            cmd.extend(args)
        
        # è¿è¡Œæµ‹è¯•
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
        
        end_time = time.time()
        duration = end_time - start_time
        
        if result.returncode == 0:
            logger.success(f"âœ… {test_file} completed successfully in {duration:.2f}s")
            if result.stdout:
                logger.info(f"Output: {result.stdout}")
            return True
        else:
            logger.error(f"âŒ {test_file} failed with return code {result.returncode}")
            if result.stderr:
                logger.error(f"Error: {result.stderr}")
            if result.stdout:
                logger.info(f"Output: {result.stdout}")
            return False
            
    except subprocess.TimeoutExpired:
        logger.error(f"â° {test_file} timed out after 5 minutes")
        return False
    except Exception as e:
        logger.error(f"ğŸ’¥ {test_file} failed with exception: {e}")
        return False

def run_basic_tests():
    """è¿è¡ŒåŸºç¡€JSONæµ‹è¯•"""
    logger.info("=== Running Basic JSON Tests ===")
    
    basic_tests = [
        "test-json.py",
        "test-json1.py", 
        "test_json.py",
        "test_json_perf.py"
    ]
    
    success_count = 0
    total_count = len(basic_tests)
    
    for test_file in basic_tests:
        if run_test_file(test_file):
            success_count += 1
    
    logger.info(f"Basic tests completed: {success_count}/{total_count} passed")
    return success_count == total_count

def run_comprehensive_tests():
    """è¿è¡Œç»¼åˆJSONæµ‹è¯•"""
    logger.info("=== Running Comprehensive JSON Tests ===")
    
    comprehensive_tests = [
        "test_json_comprehensive.py"
    ]
    
    success_count = 0
    total_count = len(comprehensive_tests)
    
    for test_file in comprehensive_tests:
        if run_test_file(test_file):
            success_count += 1
    
    logger.info(f"Comprehensive tests completed: {success_count}/{total_count} passed")
    return success_count == total_count

def run_edge_case_tests():
    """è¿è¡Œè¾¹ç•Œæƒ…å†µæµ‹è¯•"""
    logger.info("=== Running Edge Case Tests ===")
    
    edge_case_tests = [
        "test_json_edge_cases.py"
    ]
    
    success_count = 0
    total_count = len(edge_case_tests)
    
    for test_file in edge_case_tests:
        if run_test_file(test_file):
            success_count += 1
    
    logger.info(f"Edge case tests completed: {success_count}/{total_count} passed")
    return success_count == total_count

def run_aggregation_tests():
    """è¿è¡Œèšåˆæµ‹è¯•"""
    logger.info("=== Running Aggregation Tests ===")
    
    aggregation_tests = [
        "test_json_aggregation.py"
    ]
    
    success_count = 0
    total_count = len(aggregation_tests)
    
    for test_file in aggregation_tests:
        if run_test_file(test_file):
            success_count += 1
    
    logger.info(f"Aggregation tests completed: {success_count}/{total_count} passed")
    return success_count == total_count

def run_real_world_tests():
    """è¿è¡ŒçœŸå®ä¸–ç•Œåœºæ™¯æµ‹è¯•"""
    logger.info("=== Running Real-world Scenario Tests ===")
    
    real_world_tests = [
        "test_json_real_world.py"
    ]
    
    success_count = 0
    total_count = len(real_world_tests)
    
    for test_file in real_world_tests:
        if run_test_file(test_file):
            success_count += 1
    
    logger.info(f"Real-world tests completed: {success_count}/{total_count} passed")
    return success_count == total_count

def run_json_folder_tests():
    """è¿è¡Œjsonæ–‡ä»¶å¤¹ä¸­çš„æµ‹è¯•"""
    logger.info("=== Running JSON Folder Tests ===")
    
    json_folder_tests = [
        "json/test-json.py",
        "json/random_json.py"
    ]
    
    success_count = 0
    total_count = len(json_folder_tests)
    
    for test_file in json_folder_tests:
        if run_test_file(test_file):
            success_count += 1
    
    logger.info(f"JSON folder tests completed: {success_count}/{total_count} passed")
    return success_count == total_count

def run_specific_test(test_name):
    """è¿è¡Œç‰¹å®šçš„æµ‹è¯•"""
    logger.info(f"=== Running Specific Test: {test_name} ===")
    
    # æ£€æŸ¥æµ‹è¯•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    test_files = [
        test_name,
        f"{test_name}.py",
        f"test_{test_name}.py",
        f"test-{test_name}.py"
    ]
    
    for test_file in test_files:
        if os.path.exists(test_file):
            return run_test_file(test_file)
    
    logger.error(f"Test file not found: {test_name}")
    return False

def main():
    parser = argparse.ArgumentParser(description="Run JSON tests for Milvus")
    parser.add_argument("--test-type", choices=[
        "basic", "comprehensive", "edge-cases", "aggregation", 
        "real-world", "json-folder", "all"
    ], default="all", help="Type of tests to run")
    
    parser.add_argument("--specific-test", help="Run a specific test file")
    
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logger.remove()
        logger.add(sys.stderr, level="DEBUG")
    else:
        logger.remove()
        logger.add(sys.stderr, level="INFO")
    
    logger.info("ğŸš€ Starting JSON Test Runner")
    logger.info(f"Python version: {sys.version}")
    
    start_time = time.time()
    
    if args.specific_test:
        success = run_specific_test(args.specific_test)
    else:
        if args.test_type == "all":
            # è¿è¡Œæ‰€æœ‰æµ‹è¯•
            results = []
            results.append(run_basic_tests())
            results.append(run_comprehensive_tests())
            results.append(run_edge_case_tests())
            results.append(run_aggregation_tests())
            results.append(run_real_world_tests())
            results.append(run_json_folder_tests())
            success = all(results)
        elif args.test_type == "basic":
            success = run_basic_tests()
        elif args.test_type == "comprehensive":
            success = run_comprehensive_tests()
        elif args.test_type == "edge-cases":
            success = run_edge_case_tests()
        elif args.test_type == "aggregation":
            success = run_aggregation_tests()
        elif args.test_type == "real-world":
            success = run_real_world_tests()
        elif args.test_type == "json-folder":
            success = run_json_folder_tests()
    
    end_time = time.time()
    total_duration = end_time - start_time
    
    logger.info("=" * 50)
    if success:
        logger.success(f"ğŸ‰ All tests completed successfully in {total_duration:.2f}s")
        return 0
    else:
        logger.error(f"ğŸ’¥ Some tests failed. Total duration: {total_duration:.2f}s")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 